+++
+++

## Ananya Harsh Jha

Hi! I am a first-year PhD student at UW, co-supervised by [**Hannaneh Hajishirzi**](https://homes.cs.washington.edu/~hannaneh/) and [**Noah Smith**](https://nasmith.github.io/).
I am a member of the [**H2Lab**](https://h2lab.cs.washington.edu/) and [**Noah's Ark**](https://noahs-ark.github.io/). \
I am broadly interested in efficient machine learning for language.<br />

I collaborate closely with [**Emma Strubell**](https://strubell.github.io/) and various others from the [**AllenNLP**](https://allenai.org/allennlp) team at [**AI2**](https://allenai.org/), where previously, I was a [**predoctoral researcher**](https://allenai.org/predoctoral-young-investigators) supervised by [**Iz Beltagy**](https://beltagy.net/) and [**Emma Strubell**](https://strubell.github.io/).<br />

Before that, I was a research engineer at [**PyTorch Lightning**](https://github.com/Lightning-AI/lightning). I co-wrote [**TorchMetrics**](https://github.com/Lightning-AI/torchmetrics) with [**Teddy Koker**](https://teddykoker.com/) and worked on [**stochastic-autoencoders**](https://arxiv.org/pdf/2107.12329.pdf) with [**Kyunghyun Cho**](https://www.kyunghyuncho.me/).
In a previous life, I worked on [**cycle-VAEs**](https://openaccess.thecvf.com/content_ECCV_2018/papers/Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper.pdf) with [**Saket Anand**](https://faculty.iiitd.ac.in/~anands/) at [**IIIT-Delhi**](https://iiitd.ac.in/).<br />

> *If you wanna chat about research/academia/whatever, feel free to reach out to ananyahj [at] cs [dot] washington [dot] edu.*

<br />

## Representative Papers

#### conferences

- [OLMo: Accelerating the Science of Language Models](https://arxiv.org/pdf/2402.00838) \
  ACL '24 | [[code](https://github.com/allenai/OLMo)] [[website](https://allenai.org/olmo)] [[ðŸ¤— artifacts](https://huggingface.co/allenai/OLMo-7B)]

- [Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research](https://arxiv.org/pdf/2402.00159) \
  ACL '24 | [[code](https://github.com/allenai/dolma)] [[website](https://allenai.org/olmo)] [[ðŸ¤— artifacts](https://huggingface.co/datasets/allenai/dolma)]

- [Disentangling Factors of Variation with Cycle-Consistent Variational Auto-Encoders](https://openaccess.thecvf.com/content_ECCV_2018/papers/Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper.pdf) \
  ECCV '18 | [[code](https://github.com/ananyahjha93/cycle-consistent-vae)]

<!-- - [Disentangling Factors of Variation with Cycle-Consistent Variational Auto-Encoders](https://openaccess.thecvf.com/content_ECCV_2018/papers/Ananya_Harsh_Jha_Disentangling_Factors_of_ECCV_2018_paper.pdf) \
  ECCV 2018 | [[code](github.com/ananyahjha93)] [[demo](github.com/ananyahjha93)] [[slides](github.com/ananyahjha93)] [[talk](github.com/ananyahjha93)] [[website](github.com/ananyahjha93)] -->

#### preprints

- [Just CHOP: Embarrassingly Simple LLM Compression](https://arxiv.org/pdf/2305.14864v3)

- [AASAE: Augmentation-Augmented Stochastic Autoencoders](https://arxiv.org/pdf/2107.12329) \
  [[code](https://github.com/Lightning-Universe/paper-AAVAE)]

<br />

## Resources

- ðŸ“œ You can find my **grad school statement of purpose** at [**CS-SOP**](https://cs-sop.notion.site/CS-PhD-Statements-of-Purpose-df39955313834889b7ac5411c37b958d?p=2dda63bcbc8a4650bf41845f08dbb666&pm=s).
I hope this is helpful to you. Also, remember that graduate students come from diverse backgrounds, and your profile need not look like mine to be successfully accepted into the same program.

<br />

## Online Presence

- ðŸ¤“ Scholar: [Ananya Harsh Jha](https://scholar.google.com/citations?user=USRgM88AAAAJ&hl=en)
- ðŸ’» Github: [ananyahjha93@GitHub](https://github.com/ananyahjha93)
- ðŸ“„ Resume (probably not updated): [Ananya Harsh Jha](resume/resume_ananya.pdf)